# -*- coding: utf-8 -*-
"""smart_boy_bot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RFMIfht1QWbCG0Y03G_Ws0TP8dE4LnMh
"""

import os
import json
from flask import Flask, request, render_template, jsonify
from sentence_transformers import SentenceTransformer, util, InputExample, losses
from torch.utils.data import DataLoader
from pymongo import MongoClient
from werkzeug.utils import secure_filename
import docx
import openpyxl
import csv
import PyPDF2
import requests
from bs4 import BeautifulSoup
from PIL import Image
from abc import ABC, abstractmethod
from pytesseract import image_to_string
import torch

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = "uploads"
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

class DocumentRepository:
    def __init__(self):
        self.client = MongoClient(os.getenv("MONGO_URI", "mongodb://localhost:27017/"))
        self.db = self.client['smart-boy_db']
        self.collection = self.db['SmartBoy']

    def insert_document(self, text, embedding):
        self.collection.insert_one({"text": text, "embedding": embedding.tolist()})

    def find_all_documents(self):
        return self.collection.find()

    def get_all_texts(self):
        return [doc['text'] for doc in self.collection.find()]

class NLPModel:
    def __init__(self, model_name="paraphrase-MiniLM-L6-v2"):
        self.model = SentenceTransformer(model_name)

    def encode(self, text):
        return self.model.encode(text, convert_to_tensor=True)

    def fine_tune(self, train_data: list):
        examples = [InputExample(texts=[text], label=1.0) for text in train_data]
        train_dataloader = DataLoader(examples, shuffle=True, batch_size=16)
        train_loss = losses.CosineSimilarityLoss(self.model)
        self.model.fit(train_objectives=[(train_dataloader, train_loss)], epochs=1)

class BaseTextExtractor(ABC):
    @abstractmethod
    def extract_text(self, file):
        pass

class TxtTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        return file.read().decode("utf-8")

class DocxTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        doc = docx.Document(file)
        return "\n".join(para.text for para in doc.paragraphs)

class XlsxTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        workbook = openpyxl.load_workbook(file)
        sheet = workbook.active
        return "\n".join(" ".join(str(cell) for cell in row) for row in sheet.iter_rows(values_only=True))

class CsvTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        reader = csv.reader(file.read().decode('utf-8').splitlines())
        return "\n".join(" ".join(row) for row in reader)

class PdfTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        reader = PyPDF2.PdfReader(file)
        return "\n".join(page.extract_text() for page in reader.pages)

class ImageTextExtractor(BaseTextExtractor):
    def extract_text(self, file):
        try:
            image = Image.open(file)
            return image_to_string(image)
        except ImportError:
            return "OCR library (pytesseract) not installed. Cannot extract text from images."

class TextExtractorFactory:
    @staticmethod
    def get_extractor(extension):
        extractors = {
            'txt': TxtTextExtractor(),
            'docx': DocxTextExtractor(),
            'xlsx': XlsxTextExtractor(),
            'csv': CsvTextExtractor(),
            'pdf': PdfTextExtractor(),
            'jpg': ImageTextExtractor(),
            'jpeg': ImageTextExtractor(),
            'png': ImageTextExtractor(),
        }
        return extractors.get(extension)

repository = DocumentRepository()
nlp_model = NLPModel()

class DocumentService:
    @staticmethod
    def process_text(text):
        embedding = nlp_model.encode(text)
        repository.insert_document(text, embedding)
        DocumentService.fine_tune_model()
        return "Text processed and stored successfully!"

    @staticmethod
    def process_file(file):
        ext = file.filename.rsplit('.', 1)[-1].lower()
        extractor = TextExtractorFactory.get_extractor(ext)
        if extractor:
            text = extractor.extract_text(file)
            return DocumentService.process_text(text)
        return "Invalid or empty file."

    @staticmethod
    def process_link(url):
        text = DocumentService.extract_text_from_link(url)
        if text and len(text) > 0:
            embedding = nlp_model.encode(text)
            repository.insert_document(text, embedding)
            DocumentService.fine_tune_model()
            return "Link content processed and stored successfully!"
        return "No content retrieved from the URL."

    @staticmethod
    def extract_text_from_link(url):
        try:
            response = requests.get(url)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                paragraphs = soup.find_all('p')
                text = "\n".join([para.get_text() for para in paragraphs])
                return text
            else:
                return "Failed to retrieve the webpage."
        except Exception as e:
            return f"An error occurred: {e}"

    @staticmethod
    def search_answer(question):
        question_embedding = nlp_model.encode(question)
        documents = repository.find_all_documents()
        similarities = []

        for doc in documents:
            doc_embedding = doc['embedding']
            similarity = util.pytorch_cos_sim(question_embedding, torch.tensor(doc_embedding))[0][0]
            similarities.append((doc['text'], similarity))

        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[0][0] if similarities else "No relevant information found."

    @staticmethod
    def fine_tune_model():
        texts = repository.get_all_texts()
        if texts:
            nlp_model.fine_tune(texts)
            return "Model fine-tuned successfully!"
        return "No data available for fine-tuning."

@app.route('/')
def index():
    return render_template('public/index.html')

@app.route('/process_file', methods=['POST'])
def process_file():
    if 'file' not in request.files:
        return jsonify({"message": "No file uploaded."}), 400
    file = request.files['file']
    return jsonify({"message": DocumentService.process_file(file)})

@app.route('/process_text', methods=['POST'])
def process_text():
    text = request.json.get("text", "")
    if text:
        response = DocumentService.process_text(text)  # Automatic fine-tuning here
        return jsonify({"message": response}), 200
    return jsonify({"message": "No text provided."}), 400

@app.route("/process_link", methods=["POST"])
def process_link():
    url = request.json.get("url", "")
    if url:
        response = DocumentService.process_link(url)  # Automatic fine-tuning here
        return jsonify({"message": response}), 200
    return jsonify({"message": "No URL provided."}), 400

@app.route("/search_answer", methods=["POST"])
def search_answer():
    question = request.json.get("question", "")
    if question:
        answer = DocumentService.search_answer(question)
        return jsonify({"answer": answer}), 200
    return jsonify({"answer": "No question provided."}), 400

if __name__ == "__main__":
    app.run(debug=True, port=3978)
